"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[226],{3288:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"part2/chapter9-perception-vision-affordance","title":"chapter9-perception-vision-affordance","description":"Chapter 9: Perception, Vision, and Affordance Modeling","source":"@site/docs/part2/chapter9-perception-vision-affordance.md","sourceDirName":"part2","slug":"/part2/chapter9-perception-vision-affordance","permalink":"/hakathon-1/docs/part2/chapter9-perception-vision-affordance","draft":false,"unlisted":false,"editUrl":"https://github.com/physical-ai-humanoid-book/physical-ai-humanoid-book/tree/main/docs/part2/chapter9-perception-vision-affordance.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"chapter8-llm-driven-controllers","permalink":"/hakathon-1/docs/part2/chapter8-llm-driven-controllers"},"next":{"title":"chapter10-safety-constraints-compliance","permalink":"/hakathon-1/docs/part2/chapter10-safety-constraints-compliance"}}');var o=i(4848),r=i(8453);const a={},s=void 0,c={},l=[{value:"Chapter 9: Perception, Vision, and Affordance Modeling",id:"chapter-9-perception-vision-and-affordance-modeling",level:3}];function d(n){const e={em:"em",h3:"h3",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.h3,{id:"chapter-9-perception-vision-and-affordance-modeling",children:"Chapter 9: Perception, Vision, and Affordance Modeling"}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Introduction"})}),"\n",(0,o.jsxs)(e.p,{children:["Perception is the gateway through which humanoid robots acquire information about their surroundings, enabling them to understand the environment, identify objects, and infer properties necessary for intelligent interaction. Among the various sensory modalities, vision plays a preeminent role, providing rich, high-dimensional data that mirrors human visual experience. Beyond merely detecting objects, effective perception for embodied AI involves ",(0,o.jsx)(e.em,{children:"affordance modeling"}),'\u2014understanding what actions an object "affords" or allows the robot to perform. This integration of vision and affordance modeling is critical for humanoids to navigate, manipulate, and collaborate effectively in unstructured, human-centric environments.']}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Computer Vision Techniques for Humanoids"})}),"\n",(0,o.jsx)(e.p,{children:"Modern computer vision, heavily reliant on deep learning, provides humanoids with sophisticated abilities to interpret visual data:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Object Detection and Recognition:"})," Identifying and localizing specific objects within an image or video stream. Techniques like Faster R-CNN, YOLO (You Only Look Once), and SSD (Single Shot Multibox Detector) enable real-time detection of multiple objects."]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Semantic Segmentation:"})," Classifying each pixel in an image to belong to a particular object category, providing a fine-grained understanding of the scene."]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Instance Segmentation:"})," Identifying and delineating individual instances of objects, even if they are of the same class (e.g., distinguishing between two separate chairs)."]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Pose Estimation:"})," Determining the 3D position and orientation of objects or human body parts (e.g., using OpenPose for human pose estimation). This is crucial for interaction and understanding human actions."]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Depth Estimation:"})," Inferring the distance of objects from the camera, either through stereo vision (using two cameras), structured light (projecting patterns), or monocular depth estimation using deep learning models. [DIAGRAM: Flow of visual information from camera to object recognition]"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Optical Flow and Tracking:"})," Estimating the motion of objects in a video sequence, essential for predicting dynamic changes in the environment and interacting with moving targets."]}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Object Recognition and Scene Understanding"})}),"\n",(0,o.jsx)(e.p,{children:"Beyond simply identifying objects, humanoids need to comprehend the overall scene:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Contextual Reasoning:"})," Understanding how objects relate to each other and to the environment. For example, a cup on a table implies it might be used for drinking, whereas a cup in a sink implies it needs washing."]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Spatial Reasoning:"})," Inferring the 3D layout of the environment, including free space, obstacles, and the relative positions of objects."]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Activity Recognition:"})," Identifying ongoing human activities or robot tasks from visual cues, allowing for proactive assistance or collaboration."]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Material and Texture Recognition:"})," Distinguishing between different material properties (e.g., slippery, rough, fragile) which are crucial for effective manipulation and grasp planning."]}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Affordance Modeling"})}),"\n",(0,o.jsxs)(e.p,{children:["Affordance modeling is about bridging the gap between perception and action\u2014understanding what an object ",(0,o.jsx)(e.em,{children:"offers"})," in terms of interaction possibilities for the robot."]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Definition:"})," An affordance is a property of an object or environment that suggests how an agent can interact with it. For example, a handle affords grasping, a button affords pushing, a lever affords pulling."]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Learning Affordances:"})," Robots can learn affordances through various methods:","\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Self-exploration:"})," Manipulating objects and observing the effects of its actions."]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Human Demonstration (Imitation Learning):"})," Observing humans interact with objects and associating observed actions with object properties."]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Large-scale Datasets:"})," Training deep learning models on vast datasets of objects and human-object interactions annotated with affordance information."]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Representing Affordances:"}),' Affordances can be represented explicitly (e.g., a "graspable" label) or implicitly (e.g., as a probability map over an object\'s surface indicating regions suitable for grasping).']}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Integration with Planning:"}),' Affordance information directly informs action planning. If a robot perceives a "door handle" that "affords pulling," its planner can generate a pulling motion to open the door, rather than pushing or lifting.']}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"By combining advanced vision capabilities with robust affordance modeling, humanoids can move beyond rote execution to genuinely understand their physical environment and interact with it intelligently and flexibly."})]})}function h(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>s});var t=i(6540);const o={},r=t.createContext(o);function a(n){const e=t.useContext(r);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:a(n.components),t.createElement(r.Provider,{value:e},n.children)}}}]);
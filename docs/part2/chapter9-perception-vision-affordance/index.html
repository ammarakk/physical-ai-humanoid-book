<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-part2/chapter9-perception-vision-affordance" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">chapter9-perception-vision-affordance | PHYSICAL AI &amp; HUMANOID ROBOTICS</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://ammarakk.github.io/hakathon-1/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://ammarakk.github.io/hakathon-1/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://ammarakk.github.io/hakathon-1/docs/part2/chapter9-perception-vision-affordance"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="chapter9-perception-vision-affordance | PHYSICAL AI &amp; HUMANOID ROBOTICS"><meta data-rh="true" name="description" content="Chapter 9: Perception, Vision, and Affordance Modeling"><meta data-rh="true" property="og:description" content="Chapter 9: Perception, Vision, and Affordance Modeling"><link data-rh="true" rel="icon" href="/hakathon-1/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://ammarakk.github.io/hakathon-1/docs/part2/chapter9-perception-vision-affordance"><link data-rh="true" rel="alternate" href="https://ammarakk.github.io/hakathon-1/docs/part2/chapter9-perception-vision-affordance" hreflang="en"><link data-rh="true" rel="alternate" href="https://ammarakk.github.io/hakathon-1/docs/part2/chapter9-perception-vision-affordance" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"chapter9-perception-vision-affordance","item":"https://ammarakk.github.io/hakathon-1/docs/part2/chapter9-perception-vision-affordance"}]}</script><link rel="alternate" type="application/rss+xml" href="/hakathon-1/blog/rss.xml" title="PHYSICAL AI &amp; HUMANOID ROBOTICS RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/hakathon-1/blog/atom.xml" title="PHYSICAL AI &amp; HUMANOID ROBOTICS Atom Feed"><link rel="stylesheet" href="/hakathon-1/assets/css/styles.2c3303d1.css">
<script src="/hakathon-1/assets/js/runtime~main.b55601d8.js" defer="defer"></script>
<script src="/hakathon-1/assets/js/main.2f1fed19.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/hakathon-1/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/hakathon-1/"><div class="navbar__logo"><img src="/hakathon-1/img/logo.svg" alt="Physical AI Book Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/hakathon-1/img/logo.svg" alt="Physical AI Book Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">PANAVERSITY</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/hakathon-1/docs/part1/chapter1-embodied-intelligence">Book</a><a class="navbar__item navbar__link" href="/hakathon-1/chatbot">AI Chatbot</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="navbar__item"><div id="language-toggle-placeholder"></div></div><a class="navbar__item navbar__link" href="/hakathon-1/login">Log In</a><a class="navbar__item navbar__link" href="/hakathon-1/signup">Sign Up</a><a href="https://github.com/physical-ai-humanoid-book/physical-ai-humanoid-book" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/hakathon-1/docs/part1/chapter1-embodied-intelligence"><span title="Part I: Physical AI Foundations" class="categoryLinkLabel_W154">Part I: Physical AI Foundations</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/hakathon-1/docs/part2/chapter6-autonomous-decision-making"><span title="Part II: Humanoid Agent Systems" class="categoryLinkLabel_W154">Part II: Humanoid Agent Systems</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hakathon-1/docs/part2/chapter6-autonomous-decision-making"><span title="chapter6-autonomous-decision-making" class="linkLabel_WmDU">chapter6-autonomous-decision-making</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hakathon-1/docs/part2/chapter7-physical-action-planning"><span title="chapter7-physical-action-planning" class="linkLabel_WmDU">chapter7-physical-action-planning</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hakathon-1/docs/part2/chapter8-llm-driven-controllers"><span title="chapter8-llm-driven-controllers" class="linkLabel_WmDU">chapter8-llm-driven-controllers</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/hakathon-1/docs/part2/chapter9-perception-vision-affordance"><span title="chapter9-perception-vision-affordance" class="linkLabel_WmDU">chapter9-perception-vision-affordance</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/hakathon-1/docs/part2/chapter10-safety-constraints-compliance"><span title="chapter10-safety-constraints-compliance" class="linkLabel_WmDU">chapter10-safety-constraints-compliance</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/hakathon-1/docs/part3/chapter11-simulation-pipelines"><span title="Part III: Software &amp; Engineering Stack" class="categoryLinkLabel_W154">Part III: Software &amp; Engineering Stack</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/hakathon-1/docs/part4/chapter15-intelligent-learning-platform"><span title="Part IV: AI/Spec-Driven Authoring Architecture" class="categoryLinkLabel_W154">Part IV: AI/Spec-Driven Authoring Architecture</span></a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/hakathon-1/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Part II: Humanoid Agent Systems</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">chapter9-perception-vision-affordance</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>chapter9-perception-vision-affordance</h1></header><h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="chapter-9-perception-vision-and-affordance-modeling">Chapter 9: Perception, Vision, and Affordance Modeling<a href="#chapter-9-perception-vision-and-affordance-modeling" class="hash-link" aria-label="Direct link to Chapter 9: Perception, Vision, and Affordance Modeling" title="Direct link to Chapter 9: Perception, Vision, and Affordance Modeling" translate="no">​</a></h3>
<p><strong>Introduction</strong></p>
<p>Perception is the gateway through which humanoid robots acquire information about their surroundings, enabling them to understand the environment, identify objects, and infer properties necessary for intelligent interaction. Among the various sensory modalities, vision plays a preeminent role, providing rich, high-dimensional data that mirrors human visual experience. Beyond merely detecting objects, effective perception for embodied AI involves <em>affordance modeling</em>—understanding what actions an object &quot;affords&quot; or allows the robot to perform. This integration of vision and affordance modeling is critical for humanoids to navigate, manipulate, and collaborate effectively in unstructured, human-centric environments.</p>
<p><strong>Computer Vision Techniques for Humanoids</strong></p>
<p>Modern computer vision, heavily reliant on deep learning, provides humanoids with sophisticated abilities to interpret visual data:</p>
<ul>
<li class=""><strong>Object Detection and Recognition:</strong> Identifying and localizing specific objects within an image or video stream. Techniques like Faster R-CNN, YOLO (You Only Look Once), and SSD (Single Shot Multibox Detector) enable real-time detection of multiple objects.</li>
<li class=""><strong>Semantic Segmentation:</strong> Classifying each pixel in an image to belong to a particular object category, providing a fine-grained understanding of the scene.</li>
<li class=""><strong>Instance Segmentation:</strong> Identifying and delineating individual instances of objects, even if they are of the same class (e.g., distinguishing between two separate chairs).</li>
<li class=""><strong>Pose Estimation:</strong> Determining the 3D position and orientation of objects or human body parts (e.g., using OpenPose for human pose estimation). This is crucial for interaction and understanding human actions.</li>
<li class=""><strong>Depth Estimation:</strong> Inferring the distance of objects from the camera, either through stereo vision (using two cameras), structured light (projecting patterns), or monocular depth estimation using deep learning models. [DIAGRAM: Flow of visual information from camera to object recognition]</li>
<li class=""><strong>Optical Flow and Tracking:</strong> Estimating the motion of objects in a video sequence, essential for predicting dynamic changes in the environment and interacting with moving targets.</li>
</ul>
<p><strong>Object Recognition and Scene Understanding</strong></p>
<p>Beyond simply identifying objects, humanoids need to comprehend the overall scene:</p>
<ul>
<li class=""><strong>Contextual Reasoning:</strong> Understanding how objects relate to each other and to the environment. For example, a cup on a table implies it might be used for drinking, whereas a cup in a sink implies it needs washing.</li>
<li class=""><strong>Spatial Reasoning:</strong> Inferring the 3D layout of the environment, including free space, obstacles, and the relative positions of objects.</li>
<li class=""><strong>Activity Recognition:</strong> Identifying ongoing human activities or robot tasks from visual cues, allowing for proactive assistance or collaboration.</li>
<li class=""><strong>Material and Texture Recognition:</strong> Distinguishing between different material properties (e.g., slippery, rough, fragile) which are crucial for effective manipulation and grasp planning.</li>
</ul>
<p><strong>Affordance Modeling</strong></p>
<p>Affordance modeling is about bridging the gap between perception and action—understanding what an object <em>offers</em> in terms of interaction possibilities for the robot.</p>
<ul>
<li class=""><strong>Definition:</strong> An affordance is a property of an object or environment that suggests how an agent can interact with it. For example, a handle affords grasping, a button affords pushing, a lever affords pulling.</li>
<li class=""><strong>Learning Affordances:</strong> Robots can learn affordances through various methods:<!-- -->
<ul>
<li class=""><strong>Self-exploration:</strong> Manipulating objects and observing the effects of its actions.</li>
<li class=""><strong>Human Demonstration (Imitation Learning):</strong> Observing humans interact with objects and associating observed actions with object properties.</li>
<li class=""><strong>Large-scale Datasets:</strong> Training deep learning models on vast datasets of objects and human-object interactions annotated with affordance information.</li>
</ul>
</li>
<li class=""><strong>Representing Affordances:</strong> Affordances can be represented explicitly (e.g., a &quot;graspable&quot; label) or implicitly (e.g., as a probability map over an object&#x27;s surface indicating regions suitable for grasping).</li>
<li class=""><strong>Integration with Planning:</strong> Affordance information directly informs action planning. If a robot perceives a &quot;door handle&quot; that &quot;affords pulling,&quot; its planner can generate a pulling motion to open the door, rather than pushing or lifting.</li>
</ul>
<p>By combining advanced vision capabilities with robust affordance modeling, humanoids can move beyond rote execution to genuinely understand their physical environment and interact with it intelligently and flexibly.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/physical-ai-humanoid-book/physical-ai-humanoid-book/tree/main/docs/part2/chapter9-perception-vision-affordance.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/hakathon-1/docs/part2/chapter8-llm-driven-controllers"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">chapter8-llm-driven-controllers</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/hakathon-1/docs/part2/chapter10-safety-constraints-compliance"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">chapter10-safety-constraints-compliance</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#chapter-9-perception-vision-and-affordance-modeling" class="table-of-contents__link toc-highlight">Chapter 9: Perception, Vision, and Affordance Modeling</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Book Modules</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/hakathon-1/docs/intro">Introduction</a></li><li class="footer__item"><a class="footer__link-item" href="/hakathon-1/docs/part1/chapter1-embodied-intelligence">Part I: Physical AI Foundations</a></li><li class="footer__item"><a class="footer__link-item" href="/hakathon-1/docs/part2/chapter6-autonomous-decision-making">Part II: Humanoid Agent Systems</a></li><li class="footer__item"><a class="footer__link-item" href="/hakathon-1/docs/part3/chapter11-simulation-pipelines">Part III: Software &amp; Engineering Stack</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Resources</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://ai.google.dev/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Official AI Documentation<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://www.iso.org/standards/robotics.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Robotics Standards<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://arxiv.org/list/cs.RO/recent" target="_blank" rel="noopener noreferrer" class="footer__link-item">Research Papers<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/physical-ai-humanoid-book/physical-ai-humanoid-book" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://deepmind.google/technologies/gemini/" target="_blank" rel="noopener noreferrer" class="footer__link-item">AI Technology Explained<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a class="footer__link-item" href="/hakathon-1/docs/welcome">Welcome</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Physical AI & Humanoid Robotics Book. Built with Docusaurus. AI technology explained.</div></div></div></footer></div>
</body>
</html>